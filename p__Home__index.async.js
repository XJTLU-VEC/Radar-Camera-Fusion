"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[371],{19009:function(ze,B,l){l.r(B),l.d(B,{default:function(){return We}});var $=l(56690),j=l.n($),q=l(89728),S=l.n(q),ee=l(61655),R=l.n(ee),ae=l(26389),A=l.n(ae),C=l(62435),G=l(80840),ne=l(42122),a=l.n(ne),te=l(70215),_=l.n(te),re=l(66115),P=l.n(re),oe=l(38416),w=l.n(oe),F=l(84289),ie=l(43033),e=l(86074),se=["dataSource","isMobile"],ce=function(D){R()(r,D);var b=A()(r);function r(y){var t;return j()(this,r),t=b.call(this,y),w()(P()(t),"phoneClick",function(){var s=!t.state.phoneOpen;t.setState({phoneOpen:s})}),t.state={phoneOpen:!1},t}return S()(r,[{key:"render",value:function(){var t=this,s=this.props,m=s.dataSource,v=s.isMobile,f=_()(s,se),c=this.state.phoneOpen,o=m.LinkMenu,d=o.children,p=Object.keys(d).map(function(i,n){var g=d[i],x=ie.rU,h={};return g.to&&g.to.match(/\//g)&&(h.href=g.to,x="a",delete g.to),C.createElement(x,a()(a()(a()({},g),h),{},{key:n.toString()}),d[i].children)}),u=c===void 0?300:null;return(0,e.jsx)(F.ZP,a()(a()(a()({component:"header",animation:{opacity:0,type:"from"}},m.wrapper),f),{},{children:(0,e.jsxs)("div",a()(a()({},m.page),{},{className:"".concat(m.page.className).concat(c?" open":""),children:[(0,e.jsx)(F.ZP,a()(a()({animation:{x:-30,type:"from",ease:"easeOutQuad"}},m.logo),{},{children:(0,e.jsx)("img",{width:"100%",src:m.logo.children,alt:"img"})})),v&&(0,e.jsxs)("div",a()(a()({},m.mobileMenu),{},{onClick:function(){t.phoneClick()},children:[(0,e.jsx)("em",{}),(0,e.jsx)("em",{}),(0,e.jsx)("em",{})]})),(0,e.jsx)(F.ZP,a()(a()({},o),{},{animation:v?{height:0,duration:300,onComplete:function(n){t.state.phoneOpen&&(n.target.style.height="auto")},ease:"easeInOutQuad"}:null,moment:u,reverse:!!c,children:p}))]}))}))}}]),r}(C.Component),le=ce,de=l(13012),k=l.n(de),H=l(72806),T=l(1289),ue=l(72575),Ee=l(83154),me=["name","texty"],pe=function(D){R()(r,D);var b=A()(r);function r(){return j()(this,r),b.apply(this,arguments)}return S()(r,[{key:"render",value:function(){var t=Object.assign({},(k()(this.props),this.props)),s=t.dataSource;delete t.dataSource,delete t.isMobile;var m=s.textWrapper.children.map(function(v){var f=v.name,c=v.texty,o=_()(v,me);return f.match("button")?(0,e.jsx)(H.Z,a()(a()({type:"primary"},o),{},{children:v.children}),f):(0,e.jsx)("div",a()(a()({},o),{},{children:c?(0,e.jsx)(ue.Z,{type:"mask-bottom",children:v.children}):v.children}),f)});return(0,e.jsx)("div",a()(a()(a()({},t),s.wrapper),{},{children:(0,e.jsx)(T.Z,a()(a()({type:["bottom","top"],delay:200},s.textWrapper),{},{children:m}),"QueueAnim")}))}}]),r}(C.PureComponent),he=pe,ge=l(3600),I=l.n(ge),O=l(28460),ve=l(18698),fe=l.n(ve),U=/^http(s)?:\/\/([\w-]+\.)+[\w-]+(\/[\w-./?%&=]*)?/,L=function(b,r){var y=b.name.indexOf("title")===0?"h1":"div";y=b.href?"a":y;var t=typeof b.children=="string"&&b.children.match(U)?C.createElement("img",{src:b.children,alt:"img"}):b.children;return b.name.indexOf("button")===0&&fe()(b.children)==="object"&&(t=C.createElement(H.Z,a()({},b.children))),C.createElement(y,a()({key:r.toString()},b),t)},be=["childWrapper"],ye=["columns","dataSource"],xe=["dataSource","isMobile"],De=["columns","dataSource"],Be=function(D){R()(r,D);var b=A()(r);function r(){var y;j()(this,r);for(var t=arguments.length,s=new Array(t),m=0;m<t;m++)s[m]=arguments[m];return y=b.call.apply(b,[this].concat(s)),w()(P()(y),"getColumns",function(v){return v.map(function(f){var c=f.childWrapper,o=_()(f,be);return a()(a()({align:"center"},o),{},{title:(0,e.jsx)("div",a()(a()({},c),{},{children:c.children.map(L)}))})})}),w()(P()(y),"getDataSource",function(v,f){return v.map(function(c,o){var d={key:o.toString()};return c.children.forEach(function(p,u){f[u]&&(d[f[u].key]=(0,e.jsx)("div",a()(a()({},p),{},{children:typeof p.children=="string"&&p.children.match(U)?(0,e.jsx)("img",{src:p.children,alt:"img"}):p.children})))}),d})}),w()(P()(y),"getMobileChild",function(v){var f=v.columns,c=v.dataSource,o=_()(v,ye),d=f.children.filter(function(u){return u.key.indexOf("name")>=0}),p=f.children.filter(function(u){return u.key.indexOf("name")===-1});return p.map(function(u,i){var n=[].concat(d[0],u).filter(function(h){return h});n.length>1&&(n[0].colSpan=0,n[1].colSpan=2);var g=c.children.map(function(h){var z=h.children.filter(function(M){return M.name.indexOf("name")===-1}),E=h.children.filter(function(M){return M.name.indexOf("name")>=0});return a()(a()({},h),{},{children:[].concat(E[0],z[i]).filter(function(M){return M})})}),x=a()(a()({},o),{},{columns:y.getColumns(n),dataSource:y.getDataSource(g,n)});return(0,e.jsx)(O.Z,a()(a()({},x),{},{pagination:!1,bordered:!0}),i.toString())})}),y}return S()(r,[{key:"render",value:function(){var t=this.props,s=t.dataSource,m=t.isMobile,v=_()(t,xe),f=s.Table,c=s.wrapper,o=s.page,d=s.titleWrapper,p=f.columns,u=f.dataSource,i=_()(f,De),n=a()(a()({},i),{},{columns:this.getColumns(p.children),dataSource:this.getDataSource(u.children,p.children)}),g=m?this.getMobileChild(f):(0,e.jsx)(O.Z,a()(a()({},n),{},{pagination:!1,bordered:!0}),"table");return(0,e.jsx)("div",a()(a()(a()({},v),c),{},{children:(0,e.jsx)("div",a()({},o))}))}}]),r}(C.PureComponent),Ge=null,Ce=l(71230),Z=l(15746),V=l(46889),K=l.p+"static/adverse-4.9d0037aa.png",je=l.p+"static/USV.9c0b3635.png",He=function(D){R()(r,D);var b=A()(r);function r(){var y;j()(this,r);for(var t=arguments.length,s=new Array(t),m=0;m<t;m++)s[m]=arguments[m];return y=b.call.apply(b,[this].concat(s)),w()(P()(y),"getBlockChildren",function(v){return v.map(function(f){var c=Object.assign({},(k()(f),f)),o=c.title,d=c.img,p=c.content;return["title","img","content"].forEach(function(u){return delete c[u]}),(0,e.jsxs)("li",a()(a()({},c),{},{children:[(0,e.jsx)("span",a()(a()({},d),{},{children:(0,e.jsx)("img",{src:d.children,width:"100%",alt:"img"})})),(0,e.jsx)("h2",a()(a()({},o),{},{children:o.children})),(0,e.jsx)("div",a()(a()({},p),{},{children:p.children}))]}),c.name)})}),y}return S()(r,[{key:"render",value:function(){var t=Object.assign({},(k()(this.props),this.props)),s=t.dataSource,m=t.isMobile;delete t.dataSource,delete t.isMobile;var v=this.getBlockChildren(s.block.children),f=m?"bottom":"left",c=m?{y:30,opacity:0,delay:600,type:"from",ease:"easeOutQuad"}:{x:30,opacity:0,type:"from",ease:"easeOutQuad"};return(0,e.jsx)("div",a()(a()(a()({},t),s.wrapper),{},{children:(0,e.jsxs)(I(),a()(a()({},s.OverPack),{},{component:Ce.Z,children:[(0,e.jsxs)(T.Z,a()(a()({type:f,leaveReverse:!0,ease:["easeOutQuad","easeInQuad"]},s.textWrapper),{},{component:Z.Z,children:[(0,e.jsx)("div",a()(a()({},s.titleWrapper),{},{children:s.titleWrapper.children.map(L)}),"title"),(0,e.jsx)(T.Z,a()(a()({component:"ul",type:f,ease:"easeOutQuad"},s.block),{},{children:v}),"ul")]}),"text"),(0,e.jsx)(F.ZP,a()(a()({animation:c,resetStyle:!0},s.img),{},{component:Z.Z,children:(0,e.jsx)(V.Z,{src:K})}),"img")]}))}))}}]),r}(C.Component),Ue=null,Ze=l(61254),X=l(27049),W=l(95507),Se=l(45098),Ke={wrapper:{className:"home-page-wrapper content6-wrapper"},OverPack:{className:"home-page content6"},textWrapper:{className:"content6-text",xs:24,md:8},titleWrapper:{className:"title-wrapper",children:[{name:"title",children:"\u8682\u8681\u91D1\u878D\u4E91\u63D0\u4F9B\u4E13\u4E1A\u7684\u670D\u52A1",className:"title-h1"},{name:"content",className:"title-content",children:"\u57FA\u4E8E\u963F\u91CC\u4E91\u8BA1\u7B97\u5F3A\u5927\u7684\u57FA\u7840\u8D44\u6E90"}]},img:{children:"https://zos.alipayobjects.com/rmsportal/VHGOVdYyBwuyqCx.png",className:"content6-img",xs:24,md:16},block:{children:[{name:"block0",img:{children:"https://zos.alipayobjects.com/rmsportal/NKBELAOuuKbofDD.png",className:"content6-icon"},title:{className:"content6-title",children:"\u6280\u672F"},content:{className:"content6-content",children:"\u4E30\u5BCC\u7684\u6280\u672F\u7EC4\u4EF6\uFF0C\u7B80\u5355\u7EC4\u88C5\u5373\u53EF\u5FEB\u901F\u642D\u5EFA\u91D1\u878D\u7EA7\u5E94\u7528\uFF0C\u4E30\u5BCC\u7684\u6280\u672F\u7EC4\u4EF6\uFF0C\u7B80\u5355\u7EC4\u88C5\u5373\u53EF\u5FEB\u901F\u642D\u5EFA\u91D1\u878D\u7EA7\u5E94\u7528\u3002"}},{name:"block1",img:{className:"content6-icon",children:"https://zos.alipayobjects.com/rmsportal/xMSBjgxBhKfyMWX.png"},title:{className:"content6-title",children:"\u878D\u5408"},content:{className:"content6-content",children:"\u89E3\u653E\u4E1A\u52A1\u53CA\u6280\u672F\u751F\u4EA7\u529B\uFF0C\u63A8\u52A8\u91D1\u878D\u670D\u52A1\u5E95\u5C42\u521B\u65B0\uFF0C\u63A8\u52A8\u91D1\u878D\u670D\u52A1\u5E95\u5C42\u521B\u65B0\u3002\u89E3\u653E\u4E1A\u52A1\u53CA\u6280\u672F\u751F\u4EA7\u529B\uFF0C\u63A8\u52A8\u91D1\u878D\u670D\u52A1\u5E95\u5C42\u521B\u65B0\u3002"}},{name:"block2",img:{className:"content6-icon",children:"https://zos.alipayobjects.com/rmsportal/MNdlBNhmDBLuzqp.png"},title:{className:"content6-title",children:"\u5F00\u53D1"},content:{className:"content6-content",children:"\u7B26\u5408\u91D1\u878D\u53CA\u8981\u6C42\u7684\u5B89\u5168\u53EF\u9760\u3001\u9AD8\u53EF\u7528\u3001\u9AD8\u6027\u80FD\u7684\u670D\u52A1\u80FD\u529B\uFF0C\u7B26\u5408\u91D1\u878D\u53CA\u8981\u6C42\u7684\u5B89\u5168\u53EF\u9760\u3001\u9AD8\u53EF\u7528\u3001\u9AD8\u6027\u80FD\u7684\u670D\u52A1\u80FD\u529B\u3002"}}]}},Re=function(D){R()(r,D);var b=A()(r);function r(){var y;j()(this,r);for(var t=arguments.length,s=new Array(t),m=0;m<t;m++)s[m]=arguments[m];return y=b.call.apply(b,[this].concat(s)),w()(P()(y),"getBlockChildren",function(v){return v.map(function(f){var c=Object.assign({},(k()(f),f)),o=c.title,d=c.img,p=c.content;return["title","img","content"].forEach(function(u){return delete c[u]}),(0,e.jsxs)("li",a()(a()({},c),{},{children:[(0,e.jsx)("span",a()(a()({},d),{},{children:(0,e.jsx)("img",{src:d.children,width:"100%",alt:"img"})})),(0,e.jsx)("h2",a()(a()({},o),{},{children:o.children})),(0,e.jsx)("div",a()(a()({},p),{},{children:p.children}))]}),c.name)})}),y}return S()(r,[{key:"render",value:function(){var t=Object.assign({},(k()(this.props),this.props)),s=t.dataSource;delete t.dataSource,delete t.isMobile;var m=[{name:"Color, Texture, Shape",star:1},{name:"Range Measurement",star:5},{name:"Velocity Measurement",star:5},{name:"Lighting Robustness",star:5},{name:"Weather Robustness",star:5},{name:"Classification Ability",star:2},{name:"3D Perception",star:1},{name:"Cost Advantage",star:4}],v={data:m.map(function(g){return a()(a()({},g),{},{star:g.star})}),xField:"name",yField:"star",appendPadding:[0,20,0,20],color:"#B2934A",legend:!0,meta:{star:{alias:"Radar Ability",min:0,nice:!0,formatter:function(x){return x}}},xAxis:{tickLine:null},yAxis:{label:!1,grid:{alternateColor:"rgba(0, 0, 0, 0.04)"}},point:{size:2},area:{}},f=[{name:"Color, Texture, Shape",star:5},{name:"Range Measurement",star:2},{name:"Velocity Measurement",star:2},{name:"Lighting Robustness",star:3},{name:"Weather Robustness",star:3},{name:"Classification Ability",star:5},{name:"3D Perception",star:3},{name:"Cost Advantage",star:5}],c={data:f.map(function(g){return a()(a()({},g),{},{star:g.star})}),xField:"name",yField:"star",appendPadding:[0,20,0,20],color:"#B66A6A",meta:{star:{alias:"Camera Ability",min:0,nice:!0,formatter:function(x){return x}}},xAxis:{tickLine:null},yAxis:{label:!1,grid:{alternateColor:"rgba(0, 0, 0, 0.04)"}},point:{size:2},area:{}},o=[{name:"Color, Texture, Shape",star:5},{name:"Range Measurement",star:5},{name:"Velocity Measurement",star:5},{name:"Lighting Robustness",star:5},{name:"Weather Robustness",star:5},{name:"Classification Ability",star:5},{name:"3D Perception",star:3},{name:"Cost Advantage",star:4}],d={data:o.map(function(g){return a()(a()({},g),{},{star:g.star})}),xField:"name",yField:"star",color:"#589D9D",meta:{star:{alias:"Fusion Ability",min:0,nice:!0,formatter:function(x){return x}}},xAxis:{tickLine:null},yAxis:{label:!1,grid:{alternateColor:"rgba(0, 0, 0, 0.04)"}},point:{size:2},area:{}},p=function(x,h,z,E){console.log("params",x,h,z,E)},u=[{title:"Sensor",dataIndex:"name",key:"name"},{title:"Detail",dataIndex:"detail",key:"detail",render:function(x){return(0,e.jsx)("div",{dangerouslySetInnerHTML:{__html:x}})}}],i=[{key:"1",name:"Radar",detail:"Oculii EAGLE 77GHz Point Cloud Radar, 200m detection range, 0.43m range resolution, 0.27m/s velocity resolution, < 1&deg; azimuth angle resolution, < 1&deg; elevation angle resolution, 110&deg; HFOV, 45&deg; VFOV, 15Hz capture frequency"},{key:"2",name:"Camera",detail:"SONY IMX317 CMOS sensor, RGB color, 1920 x 1080 resolution, 100&deg; HFOV, 60&deg; VFOV, 30Hz capture frequency"},{key:"3",name:"GPS",detail:"latitude, longitude and altitude coordinates, < 2.5m position accu- racy, < 0.1m/s velocity accuracy, 10Hz update rate"},{key:"4",name:"IMU",detail:"l0-axis inertial navigation ARHS (3-axis gyroscope, 3-axis accelerometer, 3-axis magnetometer and a barometer), 0.5&deg; heading accuracy, 0.1&deg; roll/pitch accuracy, 50Hz update rate"}],n=[{title:"WaterScenes, the first multi-task 4D radar-camera fusion dataset on water surfaces, which offers data from multiple sensors, including a 4D radar, monocular camera, GPS, and IMU. It can be applied in multiple tasks, such as <b>object detection</b>, <b>instance segmentation</b>, <b>semantic segmentation</b>, <b>free-space segmentation</b>, and <b>waterline segmentation</b>."},{title:"Our dataset covers diverse <b>time conditions</b> (daytime, nightfall, night), <b>lighting conditions</b> (normal, dim, strong), <b>weather conditions</b> (sunny, overcast, rainy, snowy) and <b>waterway conditions</b> (river, lake, canal, moat). An information list is also offered for retrieving specific data for experiments under different conditions."},{title:"We provide <b>2D box-level</b> and <b>pixel-level</b> annotations for camera images, and <b>3D point-level</b> annotations for radar point clouds. We also offer precise timestamps for the synchronization of different sensors, as well as intrinsic and extrinsic parameters."},{title:"We provide a <b>toolkit</b> for radar point clouds that includes: pre-processing, labeling, projection and visualization, assisting researchers in processing and analyzing our dataset."}];return(0,e.jsxs)("div",{class:"home-page-wrapper content6-wrapper",children:[(0,e.jsxs)("div",{class:"ant-row home-page content6",id:"WaterScenes",children:[(0,e.jsx)(X.Z,{orientation:"center",children:(0,e.jsx)("h1",{name:"title",className:"title-h1",children:"WaterScenes Dataset"})}),(0,e.jsx)("div",{class:"ant-col content6-text ant-col-xs-24 ant-col-md-24",children:(0,e.jsx)("div",{className:"title-wrapper",children:(0,e.jsx)("div",{className:"chart",children:(0,e.jsx)(V.Z,{src:K})})})}),(0,e.jsx)("div",{class:"ant-col content6-img ant-col-xs-24 ant-col-md-24",style:{padding:"10px 0 0 0"},children:(0,e.jsx)(W.ZP,{header:(0,e.jsx)("h2",{children:"Contributions:"}),itemLayout:"horizontal",bordered:!0,dataSource:n,renderItem:function(x,h){return(0,e.jsx)(W.ZP.Item,{children:(0,e.jsx)(W.ZP.Item.Meta,{title:(0,e.jsx)("div",{dangerouslySetInnerHTML:{__html:x.title}})})})}})})]}),(0,e.jsxs)("div",{class:"ant-row home-page content6",id:"USV",children:[(0,e.jsx)(X.Z,{orientation:"center",children:(0,e.jsx)("h1",{name:"title",className:"title-h1",children:"USV Setup"})}),(0,e.jsx)("div",{class:"ant-col content6-text ant-col-xs-24 ant-col-md-12",children:(0,e.jsx)(O.Z,{pagination:{pageSize:10,hideOnSinglePage:!0},columns:u,dataSource:i})}),(0,e.jsx)("div",{class:"ant-col content6-text ant-col-xs-24 ant-col-md-11 ant-col-md-2"}),(0,e.jsx)("div",{class:"ant-col content6-text ant-col-xs-24 ant-col-md-11 ant-col-md-10",children:(0,e.jsx)(V.Z,{src:je})})]}),(0,e.jsxs)("div",{class:"ant-row home-page content6",id:"Citation",children:[(0,e.jsx)("h1",{name:"title",class:"title-h1",children:"Citation"}),(0,e.jsx)("div",{style:{backgroundColor:"#f3f6fa",padding:"10px"},children:(0,e.jsxs)("code",{children:["@misc{yao2023radarcamera,",(0,e.jsx)("br",{}),"\xA0\xA0\xA0\xA0","title={Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review}, ",(0,e.jsx)("br",{}),"\xA0\xA0\xA0\xA0","author={Shanliang Yao and Runwei Guan and Xiaoyu Huang and Zhuoxiao Li and Xiangyu Sha and Yong Yue and Eng Gee Lim and Hyungjoon Seo and Ka Lok Man and Xiaohui Zhu and Yutao Yue},",(0,e.jsx)("br",{}),"\xA0\xA0\xA0\xA0","year={2023},",(0,e.jsx)("br",{}),"\xA0\xA0\xA0\xA0","eprint={2304.10410},",(0,e.jsx)("br",{}),"\xA0\xA0\xA0\xA0","archivePrefix={arXiv},",(0,e.jsx)("br",{}),"\xA0\xA0\xA0\xA0","primaryClass={cs.CV}",(0,e.jsx)("br",{}),"}"]})})]})]})}}]),r}(C.PureComponent),Ae=Re,ke=l(67021),N=l(20550),Xe=function(D){R()(r,D);var b=A()(r);function r(){return j()(this,r),b.apply(this,arguments)}return S()(r,[{key:"render",value:function(){var t=Object.assign({},(k()(this.props),this.props)),s=t.dataSource;delete t.dataSource,delete t.isMobile;var m=[{type:"Point Cloud",x:"nuScenes",y:60},{type:"Point Cloud",x:"Astyx",y:3},{type:"Point Cloud",x:"SeeingThroughFog",y:30},{type:"Radar Tensor",x:"CARRADA",y:28},{type:"Point Cloud",x:"HawkEye",y:10},{type:"Radar Tensor",x:"Zender",y:13},{type:"Point Cloud",x:"Zender",y:13},{type:"Radar Tensor",x:"RADIATE",y:65},{type:"Point Cloud",x:"AIODrive",y:70},{type:"Radar Tensor",x:"CRUW",y:90},{type:"ADC Signal",x:"RaDICaL",y:88},{type:"Point Cloud",x:"RadarScenes",y:60},{type:"Radar Tensor",x:"RADDet",y:24},{type:"Radar Tensor",x:"FloW",y:6},{type:"Point Cloud",x:"FloW",y:6},{type:"ADC Signal",x:"RADIal",y:7},{type:"Radar Tensor",x:"RADIal",y:7},{type:"Point Cloud",x:"RADIal",y:7},{type:"Point Cloud",x:"VOD",y:21},{type:"Point Cloud",x:"Boreas",y:18},{type:"Point Cloud",x:"TJ4DRadSet",y:60},{type:"Radar Tensor",x:"K-Radar",y:55},{type:"Radar Tensor",x:"aiMotive",y:40}],v=[],f=[40,.5,13,12,3,11,44,100,400,393,49,10,4,8,8,7,40,35,26],c=[60,8,30,28,10,26,65,70,90,88,60,24,12,21,21,18,60,55,40];(0,Se.S6)(f,function(i,n){v.push({type:"text",position:[n,c[n]],content:"".concat(i,"k"),style:{textAlign:"center",fontSize:14,fill:"rgba(0,0,0,0.85)"},offsetY:-10})});var o={data:m,isStack:!0,legend:{layout:"horizontal",position:"bottom"},xAxis:{label:{autoRotate:!0,rotate:"100",offset:20}},columnWidthRatio:.5,autoFit:!0,appendPadding:[0,100,10,100],xField:"x",yField:"y",seriesField:"type",columnSize:10,color:["#9C6657","#B2934A","#5F9C6B","#5b8ff9","#5d7092","#e8684a"],label:{content:function(n){var g=parseFloat(n.value);if(g<.05)return(g*100).toFixed(1)+"%"},offset:10},tooltip:{showContent:!0,customItems:function(n){return n},formatter:function(n){return console.log(n),{name:n.type,value:"\u2705"}}},annotations:v},d=[{title:"Id",dataIndex:"key",width:"10px"},{title:"Name",dataIndex:"name",width:"10%",render:function(n,g){return(0,e.jsxs)("div",{children:[(0,e.jsx)("a",{target:"_blank",href:n[1],children:n[0]})," [",(0,e.jsx)("a",{href:"#references",children:n[2]}),"]"]})}},{title:"Year",dataIndex:"year",sorter:function(n,g){return n.year-g.year}},{title:"Task",dataIndex:"task",filters:[{text:"Object Detection",value:"Object Detection"},{text:"Semantic Segmentation",value:"Semantic Segmentation"}],onFilter:function(n,g){return g.task.includes(n)},filterSearch:!0,render:function(n,g){return(0,e.jsx)("span",{children:n.map(function(x){var h="";switch(x){case"Object Detection":h="#1890ff";break;case"Semantic Segmentation":h="#fa541c";break;case"Object Tracking":h="#fa8c16";break;case"Localization":h="#13c2c2";break;case"Planning":h="#52c41a";break;case"Prediction":h="#f5222d";break;case"":h="#722ed1";break;case"":h="#eb2f96";break;case"":h="#722ed1";break;default:h="blue-inverse"}return(0,e.jsx)(N.Z,{color:h,children:x},x)})})}},{title:"Annotation",dataIndex:"annotation",filters:[{text:"2D box-level",value:"2D box-level"},{text:"3D box-level",value:"3D box-level"},{text:"2D pixel-level",value:"2D pixel-level"},{text:"3D point-level",value:"3D point-level"}],onFilter:function(n,g){return g.annotation.includes(n)},filterSearch:!0,render:function(n,g){return(0,e.jsx)("span",{children:n.map(function(x){var h="";switch(x){case"2D box-level":h="#1890ff";break;case"3D box-level":h="#fa541c";break;case"2D pixel-level":h="#fa8c16";break;case"2D point-level":h="#13c2c2";break;default:h="blue-inverse"}return(0,e.jsx)(N.Z,{color:h,children:x},x)})})}},{title:"Radar Data Representation",dataIndex:"radar_data_representation",filters:[{text:"Point Cloud",value:"Point Cloud"},{text:"Range-Doppler Tensor",value:"Range-Doppler Tensor"},{text:"Range-Azimuth Tensor",value:"Range-Azimuth Tensor"},{text:"Range-Azimuth-Doppler Tensor",value:"Range-Azimuth-Doppler Tensor"}],onFilter:function(n,g){return g.radar_data_representation.includes(n)},filterSearch:!0,render:function(n,g){return(0,e.jsx)("span",{children:n.map(function(x){var h="";switch(x){case"Point Cloud":h="#108ee9";break;case"ADC Signal":h="#f50";break;case"Range-Doppler Tensor":h="#2db7f5";case"Range-Azimuth Tensor":h="#2db7f5";case"Range-Azimuth-Doppler Tensor":h="#2db7f5";break;case"Grid Map":h="#87d068";break;default:h="#108ee9"}return(0,e.jsx)(N.Z,{color:h,children:x},x)})})}},{title:"Category Number",dataIndex:"category_number"},{title:"Categories",dataIndex:"categories"},O.Z.EXPAND_COLUMN,{title:"Record Area",dataIndex:"record_area"},{title:"Record Time",dataIndex:"record_time"},{title:"Affiliation",dataIndex:"affiliation"}],p=[{key:"1",name:["nuScenes","https://www.nuscenes.org/nuscenes","1"],year:2019,task:["Object Detection","Object Tracking","Localization","Planning","Prediction"],annotation:["3D box-level"],radar_data_representation:["Point Cloud"],category_number:23,categories:"Vehicle, Pedestrian, Bicycle, Movable Object, Static Object, etc.",size:"1000 scenes, 1.4M boxes, 40k frames, 5.5 hours",scenarios:["A diverse set of locations (urban, residential, nature and industrial), times (day and night)","sun, rain and clouds"],record_area:"Boston, Singapore",record_time:"September 2018",affiliation:"nuTonomy"},{key:"2",name:["Astyx","http://www.astyx.net","2"],year:2019,task:["Object Detection"],annotation:["3D box-level"],radar_data_representation:["Point Cloud"],category_number:7,categories:"Bus, Car, Cyclist, Motorcyclist, Person, Trailer, Truck",size:"500 frames, around 3000 labeled objects",scenarios:["-"],record_area:"South of Germany",record_time:"-",affiliation:"Technical University of Munich"},{key:"3",name:["SeeingThroughFog","https://www.uni-ulm.de/en/in/driveu/projects/dense-datasets/","3"],year:2020,task:["Object Detection"],annotation:["2D box-level","3D box-level"],radar_data_representation:["Point Cloud"],category_number:4,categories:"Passenger Car, Large Vehicle, Pedestrian, Ridable Vehicle",size:"12k samples in real-world driving scenes and 1.5k samples in controlled weather conditions within a fog chamber, 100k objects",scenarios:["Pedestrian zone, residential area, construction area and highway, daytime and street condition","under all weather conditions. Severe weather \u2013 such as snow, heavy rain or fog"],record_area:"Germany, Sweden, Denmark, and Finland",record_time:"February and December 2019",affiliation:"Mercedes-Benz AG"},{key:"4",name:["CARRADA","https://arthurouaknine.github.io/codeanddata/carrada","4"],year:2020,task:["Object Detection","Semantic Segmentation","Object Tracking","Trajectory Prediction"],annotation:["2D box-level","2D pixel-level"],radar_data_representation:["Range-Doppler Tensor","Range-Azimuth Tensor"],category_number:3,categories:"Pedestrian, Car, Cyclist",size:"12,666 frames, 78 instances, 7,139 annotated frames with instances, 23GB synchronized camera and radar views",scenarios:["Urban driving scenarios","adverse weather conditions"],record_area:"Canada",record_time:"-",affiliation:"T\xE9l\xE9com Paris"},{key:"5",name:["HawkEye","https://jguan.page/HawkEye/","5"],year:2020,task:["Semantic Segmentation"],annotation:["3D point-level"],radar_data_representation:["Point Cloud"],category_number:9,categories:"Sub-compact, Compact, Mid-sized, Full-sized, Sports, SUVs, Jeep, Vans, Trucks",size:"3k images, 4k scenes, 120 car models",scenarios:["327 scenes of cars in 3 types of backgrounds: indoor parking garage, outdoor lot, and outdoor house drive-through."],record_area:"-",record_time:"-",affiliation:"University of Illinois at Urbana-Champaign"},{key:"6",name:["Zendar","http://zendar.io/dataset","6"],year:2020,task:["Object Detection","Mapping","Localization"],annotation:["2D box-level"],radar_data_representation:["Range-Doppler Tensor","Range-Azimuth Tensor","Point Cloud"],category_number:1,categories:"Car",size:"Over 11k moving cars labeled in 27 diverse scenes with over 40k automatically generated labels",scenarios:["Complex urban driving scenarios"],record_area:"-",record_time:"-",affiliation:"Zendar"},{key:"7",name:["RADIATE","http://pro.hw.ac.uk/radiate/","7"],year:2020,task:["Object Detection","Object Tracking","SLAM","Scene Understanding"],annotation:["2D box-level"],radar_data_representation:["Range-Azimuth Tensor"],category_number:8,categories:"Car, Van, Bus, Truck, Motorbike, Bicycle, Pedestrian and a group of pedestrians",size:"200k bounding boxes over 44k radar frames",scenarios:["driving scenarios (e.g., parked, urban, motorway and suburban)","a variety of weather conditions (e.g., sun, night, rain, fog and snow)"],record_area:"Edinburgh",record_time:"Between February 2019 and February 2020",affiliation:"Heriot-Watt University"},{key:"8",name:["AIODrive","http://www.aiodrive.org/","8"],year:2020,task:["Object Detection","Object Tracking","Semantic Segmentation","Trajectory Prediction","Depth Estimation"],annotation:["2D box-level","3D box-level"],radar_data_representation:["Point Cloud"],category_number:11,categories:"Vehicle, Pedestrian, Vegetation, Building, Road, Sidewalk, Wall, Traffic Sign, Pole and Fence",size:"500k annotated images for 5 camera viewpoints, 100k annotated frames for radar sensor",scenarios:["Crowded scenes, people running, high-speed driving, violations of the traffic rule, and car accidents.","Adverse weather and lighting."],record_area:"one of eight cities from Carla assets",record_time:"-",affiliation:"Carnegie Mellon University"},{key:"9",name:["CRUW","https://www.cruwdataset.org/","9"],year:2021,task:["Object Detection"],annotation:["2D box-level"],radar_data_representation:["Range-Azimuth Tensor"],category_number:3,categories:"Pedestrian, Cyclist, Car",size:"400K frames, 260K objects, 3.5 hours",scenarios:["Area: parking lot, campus road, city street, and highway. Several vision-fail scenarios where the image qualities are pretty bad, i.e., dark, strong light, blur, etc.","strong/weak lighting condition"],record_area:"-",record_time:"-",affiliation:"University of Washington"},{key:"10",name:["RaDICaL","https://publish.illinois.edu/radicaldata/","10"],year:2021,task:["Object Detection"],annotation:["2D box-level"],radar_data_representation:["ADC Signal"],category_number:2,categories:"Pedestrian, Car",size:"393k frames",scenarios:["Indoor: people, static clutter; outdoor: neighborhood, suburban, highways and city roads."],record_area:"-",record_time:"-",affiliation:"University of Illinois at Urbana-Champaign"},{key:"11",name:["RadarScenes","https://radar-scenes.com/","11"],year:2021,task:["Object Detection","Semantic Segmentation"],annotation:["2D pixel-level","3D point-level"],radar_data_representation:["Point Cloud"],category_number:11,categories:"Car, Large Vehicle, Truck, Bus, Train, Bicycle, Motorized Two-wheeler, Pedestrian, Pedestrian Group, Animal, and Other",size:"40.208 frames, 158 individual sequences, 118.9M radar points",scenarios:["Inner city, T-junction, commercial area, urban area, country road, road works"],record_area:"Ulm, Germany",record_time:"Between 2016 and 2018",affiliation:"Mercedes-Benz AG, Stuttgart, Germany"},{key:"12",name:["RADDet","https://github.com/ZhangAoCanada/RADDet","12"],year:2021,task:["Object Detection"],annotation:["2D box-level","3D box-level"],radar_data_representation:["Range-Azimuth-Doppler Tensor"],category_number:6,categories:"Person, Bicycle, Car, Motorcycle, Bus, Truck",size:"10,158 frames",scenarios:["Sidewalks","sunny weather conditions"],record_area:"-",record_time:"September to October 2020",affiliation:"University of Ottawa"},{key:"13",name:["FloW","https://github.com/ORCA-Uboat/FloW-Dataset","13"],year:2021,task:["Object Detection"],annotation:["2D box-level"],radar_data_representation:["Range-Doppler Tensor","Point Cloud"],category_number:1,categories:"Bottle",size:"4k frames",scenarios:["Inland water surface"],record_area:"-",record_time:"-",affiliation:"ORCA-Uboat"},{key:"14",name:["RADIal","https://github.com/valeoai/RADIal","14"],year:2021,task:["Object Detection","Semantic Segmentation"],annotation:["2D box-level"],radar_data_representation:["ADC Signal","Range-Azimuth-Doppler Tensor","Range-Azimuth Tensor","Range-Doppler Tensor","Point Cloud"],category_number:1,categories:"Vehicle",size:"8,252 frames are labelled with 9,550 vehicle",scenarios:["City street, highway, countryside road"],record_area:"-",record_time:"-",affiliation:"Valeo.ai, Paris, France"},{key:"15",name:["VoD","https://tudelft-iv.github.io/view-of-delft-dataset/","15"],year:2022,task:["Object Detection"],annotation:["2D box-level","3D box-level"],radar_data_representation:["Point Cloud"],category_number:13,categories:"Car, Pedestrian, Cyclist, Rider, Unused Bicycle, Bicycle Rack, Human Depiction, Moped or Scooter, Motor, Ride Other, Vehicle Other, Truck, Ride Uncertain",size:"8693 frames, 123,106 annotations of both moving and static objects, including 26,587 pedestrian, 10,800 cyclist and 26,949 car labels",scenarios:["Campus, suburb and old-town locations. With a preference for scenarios containing vulnerable road users"],record_area:"City of Delft (The Netherlands)",record_time:"-",affiliation:"TU Delft, The Netherlands"},{key:"16",name:["Boreas","https://www.boreas.utias.utoronto.ca/","16"],year:2022,task:["Object Detection","Localization","Odometry"],annotation:["2D box-level"],radar_data_representation:["Range-Azimuth Tensor"],category_number:4,categories:"Car, Pedestrian, Cyclist, Misc",size:"7.1k frames for detection, over 350km of driving data, 326,180 unique 3D box annotations",scenarios:["a repeated route near the University of Toronto Institute for Aerospace Studies (UTIAS)","various weather conditions (sun, cloud, rain, night, snow) and seasons."],record_area:"University of Toronto Institute for Aerospace Studies (UTIAS)",record_time:"November, 2020 and \uFB01nishing in November, 2021",affiliation:"University of Toronto"},{key:"17",name:["TJ4DRadSet","https://github.com/TJRadarLab/TJ4DRadSet","17"],year:2022,task:["Object Detection","Object Tracking"],annotation:["3D box-level"],radar_data_representation:["Point Cloud"],category_number:8,categories:"Car, Pedestrian, Cyclist, Bus, Motorcyclist, Truck, Engineering Vehicle, Tricyclist",size:"40K frames in total, 7757 frames within 44 consecutive sequences",scenarios:["various driving scenarios, different road types, such as urban roads, elevated roads, industrial zones, etc.","various lighting conditions, such as normal lighting, bright light and darkness, and different road types, such as urban roads, elevated roads, industrial zones, etc. Complex scenarios such as object-dense intersections, and simple scenarios such as one-way streets with a few objects."],record_area:"Suzhou, China",record_time:"Fourth quarter of 2021",affiliation:"Tongji University"},{key:"18",name:["K-Radar","https://github.com/kaist-avelab/k-radar","18"],year:2022,task:["Object Detection","Object Tracking","SLAM"],annotation:["3D box-level"],radar_data_representation:["Range-Azimuth-Doppler Tensor"],category_number:5,categories:"Pedestrian, Motobike, Bicycle, Sedan, Bus or Truck",size:"35K frames of 4D radar tensor",scenarios:["adverse weathers (fog, rain, and snow)","various road structures (urban, suburban roads, alleyways, and highways)."],record_area:"Daejeon of the Republic of Korea",record_time:"-",affiliation:"KAIST"},{key:"19",name:["aiMotive","https://github.com/aimotive/aimotive_dataset","19"],year:2022,task:["Object Detection"],annotation:["3D box-level"],radar_data_representation:["point cloud"],category_number:14,categories:"Pedestrian, Car, Bus, Truck, Van, Motorcycle, Pickup, Rider, Bicycle, Trailer, Train, Shopping Cart, Other Object",size:"26,583 frames, 425k objects",scenarios:["a diverse set of locations(highway, suburban, urban), times(daytime, night), and weather conditions(sun, cloud, rain, glare).","highway, urban, and suburban areas"],record_area:"California, US; Austria; and Hungary ",record_time:"-",affiliation:"aimotive"}],u=function(n,g,x,h){console.log("params",n,g,x,h)};return(0,e.jsx)("div",a()(a()(a()({},t),s.wrapper),{},{id:"datasets",children:(0,e.jsxs)("div",{className:"title-wrapper",children:[(0,e.jsxs)("div",{className:"chart",children:[(0,e.jsx)("h2",{name:"title",className:"title-h2",children:"Radar-Camera Fusion Datasets"}),(0,e.jsx)(ke.Z,a()(a()({},o),{},{style:{textAlign:"center"}}))]}),(0,e.jsx)("br",{}),(0,e.jsx)("br",{}),(0,e.jsx)("br",{}),(0,e.jsx)(O.Z,{bordered:!0,scroll:{x:"200px"},pagination:{pageSize:10,hideOnSinglePage:!0},columns:d,dataSource:p,onChange:u,expandable:{columnTitle:"Size / Scenarios",expandedRowRender:function(n){return(0,e.jsxs)("p",{style:{margin:0},children:["Size:  ",n.size,(0,e.jsx)("br",{}),"Scenarios:  ",n.scenarios]})},rowExpandable:function(n){return n.name!=="Not Expandable"}}})]})}))}}]),r}(C.PureComponent),Ye=null,Je=function(D){R()(r,D);var b=A()(r);function r(){return j()(this,r),b.apply(this,arguments)}return S()(r,[{key:"render",value:function(){var t=Object.assign({},(k()(this.props),this.props)),s=t.dataSource;delete t.dataSource,delete t.isMobile;var m=[{title:"Id",dataIndex:"key",width:"10px"},{title:"Name",dataIndex:"name",width:"10%",render:function(o,d){var p=o.toString().split(",");return(0,e.jsxs)("div",{children:[p[0]," [",(0,e.jsx)("a",{href:"#references",children:parseInt(p[1].trim())+19}),"]"]})}},{title:"Short Name",dataIndex:"short_name"},{title:"Year",dataIndex:"year",sorter:function(o,d){return o.year-d.year}},{title:"Task",dataIndex:"task",filters:[{text:"Object Detection",value:"Object Detection"},{text:"Semantic Segmentation",value:"Semantic Segmentation"}],onFilter:function(o,d){return d.task.includes(o)},filterSearch:!0,width:"10%",render:function(o,d){var p=o.toString().split("|");console.log(p);var u=[];return p.map(function(i){i=i.trim();var n="";switch(i){case"Object Detection":n="#1890ff";break;case"Semantic Segmentation":n="#fa541c";break;case"Object Tracking":n="#fa8c16";break;case"Localization":n="#13c2c2";break;case"Planning":n="#52c41a";break;case"Prediction":n="#f5222d";break;case"Object Classification":n="#eb2f96";break;default:n="blue-inverse"}u.push((0,e.jsx)(N.Z,{color:n,children:i},i))}),u}},{title:"Annotation",dataIndex:"annotation",filters:[{text:"2D box-level",value:"2D box-level"},{text:"3D box-level",value:"3D box-level"},{text:"2D pixel-level",value:"2D pixel-level"},{text:"3D point-level",value:"3D point-level"}],onFilter:function(o,d){return d.annotation.includes(o)},filterSearch:!0,render:function(o,d){var p=o.toString().split("|"),u=[];return p.map(function(i){i=i.trim();var n="";switch(i){case"2D box-level":n="#1890ff";break;case"3D box-level":n="#fa541c";break;case"2D pixel-level":n="#fa8c16";break;case"2D point-level":n="#13c2c2";break;default:n="blue-inverse"}u.push((0,e.jsx)(N.Z,{color:n,children:i},i))}),u}},{title:"Radar Data Representation",dataIndex:"radar_data_representation",filters:[{text:"Point Cloud",value:"Point Cloud"},{text:"Range-Doppler Tensor",value:"Range-Doppler Tensor"},{text:"Range-Azimuth Tensor",value:"Range-Azimuth Tensor"},{text:"Range-Azimuth-Doppler Tensor",value:"Range-Azimuth-Doppler Tensor"}],onFilter:function(o,d){return d.radar_data_representation.includes(o)},filterSearch:!0,render:function(o,d){var p=o.toString().split("|"),u=[];return p.map(function(i){i=i.trim();var n="";switch(i){case"Point Cloud":n="#108ee9";break;case"ADC Signal":n="#f50";break;case"Range-Doppler Tensor":n="#2db7f5";case"Range-Azimuth Tensor":n="#2db7f5";case"Range-Azimuth-Doppler Tensor":n="#2db7f5";default:n="blue-inverse"}u.push((0,e.jsx)(N.Z,{color:n,children:i},i))}),u}},{title:"Fusion Level",dataIndex:"fusion_level",filters:[{text:"Object Level",value:"Object Level"},{text:"Data Level",value:"Data Level"},{text:"Feature Level",value:"Feature Level"},{text:"Mixed Level",value:"Mixed Level"}],onFilter:function(o,d){return d.fusion_level.includes(o)},filterSearch:!0,render:function(o,d){var p=o.toString().split("|"),u=[];return p.map(function(i){i=i.trim();var n="";switch(i){case"Object Level":n="#1890ff";break;case"Data Level":n="#fa541c";break;case"Feature Level ":n="#fa8c16";break;case"Mixed Level":n="#13c2c2";break;default:n="blue-inverse"}u.push((0,e.jsx)(N.Z,{color:n,children:i},i))}),u}},{title:"Fusion Operation",dataIndex:"fusion_operation",render:function(o,d){for(var p=o.toString().split("|"),u="",i=0;i<p.length;i++)i==0?u=p[i]:u=(0,e.jsxs)("span",{children:[u,(0,e.jsx)("br",{}),p[i]]});return(0,e.jsx)("div",{children:u})}},{title:"Network",dataIndex:"network"},{title:"Projection",dataIndex:"projection"},{title:"Dataset",dataIndex:"dataset",filters:[{text:"nuScenes",value:"nuScenes"}],onFilter:function(o,d){return d.dataset.includes(o)},filterSearch:!0,render:function(o,d){var p=o.toString().split("|"),u=[];return p.map(function(i){u.push((0,e.jsxs)("div",{children:[(0,e.jsx)("span",{children:i}),(0,e.jsx)("br",{})]}))}),u}},{title:"Evaluation Metrics",dataIndex:"evaluation_metrics",render:function(o,d){var p=o.toString().split("|"),u=[];return p.map(function(i){u.push((0,e.jsxs)("div",{children:[(0,e.jsx)("span",{children:i}),(0,e.jsx)("br",{})]}))}),u}},{title:"Conference/Journal",dataIndex:"conference_journal"},{title:"Source Code",dataIndex:"source_code",render:function(o,d){if(console.log(o),o!="-")return(0,e.jsx)("a",{target:"_blank",href:o,children:o})}}],v=[{key:"1",name:"Distant vehicle detection using radar and vision, 1",short_name:"-",year:2019,task:"Object Detection",annotation:"2D box-level",radar_data_representation:"Point Cloud",projection:"Radar point to image plane",fusion_level:"Feature Level",fusion_operation:"Addition | Concatenation",network:"One-stage network based on ResNet",dataset:"Self-Recorded",evaluation_metrics:"AP",conference_journal:"2019 International Conference on Robotics and Automation (ICRA)",source_code:"-"},{key:"2",name:"RRPN: Radar Region Proposal Network for Object Detection in Autonomous Vehicles, 2",short_name:"RRPN",year:2019,task:"Object Detection",annotation:"2D box-level",radar_data_representation:"Point Cloud",projection:"Radar proposal to image plane",fusion_level:"Data Level",fusion_operation:"Transformation matrix",network:"RRPN",dataset:"nuScenes",evaluation_metrics:"AP | AR",conference_journal:"ICIP",source_code:"https://github.com/mrnabati/RRPN"},{key:"3",name:"Object Detection and Identification using Vision and Radar Data Fusion System for Ground-based Navigation, 3",short_name:"-",year:2019,task:"Object Detection",annotation:"2D box-level",radar_data_representation:"Point Cloud",projection:"Radar point to image plane",fusion_level:"Object Level",fusion_operation:"Transformation matrix",network:"YOLOv3",dataset:"Self-Recorded",evaluation_metrics:"-",conference_journal:"2019 6th International Conference on Signal Processing and Integrated Networks (SPIN)",source_code:"-"},{key:"4",name:"Automotive radar and camera fusion using Generative Adversarial Networks, 4",short_name:"CMGGAN",year:2019,task:"Semantic Segmentation",annotation:"2D point-level",radar_data_representation:"Grid Map",projection:"-",fusion_level:"Feature Level",fusion_operation:"Addition",network:"CMGGAN",dataset:"Self-Recorded",evaluation_metrics:"TP",conference_journal:"Elsevier CVIU (Computer Vision and Image Understanding)",source_code:"-"},{key:"5",name:"Deep Learning Based 3D Object Detection for Automotive Radar and Camera, 5",short_name:"-",year:2019,task:"Object Detection",annotation:"3D box-level",radar_data_representation:"Point Cloud",projection:"Radar point to BEV",fusion_level:"Data Level",fusion_operation:"Transformation matrix",network:"A 3D region proposal network based on VGG",dataset:"Astyx",evaluation_metrics:"AP | PRC",conference_journal:"16th European Radar Conference",source_code:"-"},{key:"6",name:"RVNet: Deep Sensor Fusion of Monocular Camera and Radar for Image-Based Obstacle Detection in Challenging Environments, 6",short_name:"RVNet",year:2019,task:"Object Detection",annotation:"2D box-level",radar_data_representation:"Point Cloud",projection:"Radar point to image plane",fusion_level:"Feature Level",fusion_operation:"Concatenation",network:"RVNet based on YOLOv3",dataset:"nuScenes",evaluation_metrics:"AP | mAP",conference_journal:"PSIVT 2019 (Pacific-Rim Symposium on Image and Video Technology)",source_code:"-"},{key:"7",name:"Radar and Camera Early Fusion for Vehicle Detection in Advanced Driver Assistance Systems, 7",short_name:"FusionNet",year:2019,task:"Object Detection | Object Classification",annotation:"2D box-level",radar_data_representation:"Range-Azimuth Tensor",projection:"Image to BEV",fusion_level:"Feature-Level",fusion_operation:"Concatenation",network:"FusionNet inspired by SSD",dataset:"Self-Recorded",evaluation_metrics:"mAP",conference_journal:"33rd Conference on Neural Information Processing Systems",source_code:"-"},{key:"8",name:"SO-Net: Joint Semantic Segmentation and Obstacle Detection Using Deep Fusion of Monocular Camera and Radar, 8",short_name:"SO-Net",year:2020,task:"Object Detection | Semantic Segmentation",annotation:"2D box-level | 2D pixel-level",radar_data_representation:"Point Cloud",projection:"Radar point to image plane",fusion_level:"Feature Level",fusion_operation:"Concatenation",network:"SO-Net based on the RVNet",dataset:"nuScenes",evaluation_metrics:"AP",conference_journal:"PSIVT 2019 (Pacific-Rim Symposium on Image and Video Technology)",source_code:"-"},{key:"9",name:"Spatial Attention Fusion for Obstacle Detection Using MmWave Radar and Vision Sensor, 9",short_name:"SAF-FCOS",year:2020,task:"Object Detection",annotation:"2D box-level",radar_data_representation:"Point Cloud",projection:"Radar point to image plane",fusion_level:"Feature Level",fusion_operation:"Addition | Multiplication",network:"SAF based on FCOS",dataset:"nuScenes",evaluation_metrics:"AP",conference_journal:"Sensors",source_code:"https://github.com/Singingkettle/SAF-FCOS"},{key:"10",name:"A Deep learning-based radar and camera sensor fusion architecture for object detection, 10",short_name:"CRF-Net",year:2019,task:"Object Detection",annotation:"2D box-level",radar_data_representation:"Point Cloud",projection:"Radar point to image plane",fusion_level:"Data Level",fusion_operation:"Concatenation",network:"CRF-Net based on RetinaNet",dataset:"nuScenes | Self-Recorded",evaluation_metrics:"mAP",conference_journal:"SDF",source_code:"https://github.com/TUMFTM/CameraRadarFusionNet"},{key:"11",name:"Seeing Through Fog Without Seeing Fog: Deep Multimodal Sensor Fusion in Unseen Adverse Weather, 11",short_name:"-",year:2020,task:"Object Detection",annotation:"2D box-level",radar_data_representation:"Point Cloud",projection:"Radar point to image plane",fusion_level:"Feature Level",fusion_operation:"Concatenation | Attention",network:"A modified VGG backbone and SSD blocks",dataset:"DENSE",evaluation_metrics:"AP",conference_journal:"CVPR 2020",source_code:"https://github.com/princeton-computational-imaging/SeeingThroughFog"},{key:"12",name:"Radar+RGB Attentive Fusion for Robust Object Detection in Autonomous Vehicles, 12",short_name:"BIRANet",year:2020,task:"Object Detection",annotation:"2D box-level",radar_data_representation:"Point Cloud",projection:"-",fusion_level:"Feature Level",fusion_operation:"Addition",network:"RANet and BIRANet based on ResNet",dataset:"nuScenes",evaluation_metrics:"-",conference_journal:"arXiv",source_code:"https://github.com/RituYadav92/Radar-RGB-Attentive-Multimodal-Object-Detection"},{key:"13",name:"Radar-Camera Sensor Fusion for Joint Object Detection and Distance Estimation in Autonomous Vehicles, 13",short_name:"-",year:2020,task:"Object Detection | Depth Estimation",annotation:"2D box-level",radar_data_representation:"Point Cloud",projection:"Radar proposal to image plane",fusion_level:"Mixed Level",fusion_operation:"-",network:"FPN with ResNet as backbone, and RPN in Faster R-CNN",dataset:"nuScenes",evaluation_metrics:"AP | AR",conference_journal:"arXiv",source_code:"-"},{key:"14",name:"YOdar: Uncertainty-based Sensor Fusion for Vehicle Detection with Camera and Radar Sensors, 14",short_name:"YOdar",year:2020,task:"Object Detection",annotation:"2D box-level",radar_data_representation:"Point Cloud",projection:"Radar point to image plane",fusion_level:"Feature Level",fusion_operation:"Concatenation",network:"YOdar based on YOLOv3",dataset:"nuScenes",evaluation_metrics:"AP | mAP",conference_journal:"arXiv",source_code:"-"},{key:"15",name:"CenterFusion: Center-based Radar and Camera Fusion for 3D Object Detection, 15",short_name:"CenterNet",year:2020,task:"Object Detection",annotation:"3D box-level",radar_data_representation:"Point Cloud",projection:"Radar point to image plane",fusion_level:"Feature Level",fusion_operation:"Concatenation",network:"CenterNet with DLA backbone",dataset:"nuScenes",evaluation_metrics:"mAP",conference_journal:"WACV 2021",source_code:"https://github.com/mrnabati/CenterFusion"},{key:"16",name:"RODNet: Radar Object Detection Using Cross-Modal Supervision, 16",short_name:"RODNet",year:2020,task:"Object Detection",annotation:"2D box-level",radar_data_representation:"Range-Azimuth Tensor",projection:"Camera image to radar range-azimuth coordinates",fusion_level:"Feature Level",fusion_operation:"-",network:"RODNet",dataset:"CRUW",evaluation_metrics:"AP | AR | OLS",conference_journal:"WACV 2021",source_code:"https://github.com/yizhou-wang/RODNet"},{key:"17",name:"RAMP-CNN: A Novel Neural Network for Enhanced Automotive Radar Object Recognition, 17",short_name:"RAMP-CNN",year:2021,task:"Object Detection",annotation:"2D box-level",radar_data_representation:"Range-Azimuth-Doppler Tensor",projection:"-",fusion_level:"Feature Level",fusion_operation:"Concatenation",network:"RAMP-CNN",dataset:"CRUW",evaluation_metrics:"-",conference_journal:"IEEE Sensors",source_code:"-"},{key:"18",name:"A Feature Pyramid Fusion Detection Algorithm Based on Radar and Camera Sensor, 18",short_name:"-",year:2021,task:"Object Detection",annotation:"3D box-level",radar_data_representation:"Point Cloud",projection:"Radar point to image plane",fusion_level:"Feature Level",fusion_operation:"Multiplication | Concatenation",network:"A network based on YOLOv3",dataset:"nuScenes",evaluation_metrics:"-",conference_journal:"ICSP 2020",source_code:"-"},{key:"19",name:"Low-level Sensor Fusion Network for 3D Vehicle Detection using Radar Range-Azimuth Heatmap and Monocular Image, 19",short_name:"-",year:2020,task:"Object Detection",annotation:"3D box-level",radar_data_representation:"Range-Azimuth Tensor",projection:"-",fusion_level:"Feature Level",fusion_operation:"Concatenation",network:"A network based on VGG and FPN",dataset:"Self-Recorded",evaluation_metrics:"-",conference_journal:"ACCV 2020",source_code:"-"},{key:"20",name:"Radar Camera Fusion via Representation Learning in Autonomous Driving, 20",short_name:"AssociationNet",year:2021,task:"Object Detection",annotation:"2D box-level",radar_data_representation:"Point Cloud",projection:"Radar point to image plane",fusion_level:"Object Level",fusion_operation:"Transformation matrix | Concatenation",network:"AssociationNet",dataset:"Self-Recorded",evaluation_metrics:"-",conference_journal:"WACV 2021",source_code:"-"},{key:"21",name:"Radar Voxel Fusion for 3D Object Detection, 21",short_name:"RVF-Net",year:2021,task:"Object Detection",annotation:"3D box-level",radar_data_representation:"Point Cloud",projection:"-",fusion_level:"Data Level",fusion_operation:"Concatenation",network:"RVF-Net based on VoxelNet",dataset:"nuScenes",evaluation_metrics:"AP",conference_journal:"MDPI Apply Science",source_code:"-"},{key:"22",name:"3D Detection and Tracking for On-road Vehicles with a Monovision Camera and Dual Low-cost 4D mmWave Radars, 22",short_name:"-",year:2021,task:"Object Detection",annotation:"3D box-level",radar_data_representation:"Point Cloud",projection:"Radar point to image plane | Radar point to BEV",fusion_level:"Mixed Level",fusion_operation:"Concatenation",network:"CNN with SSMA block",dataset:"Astyx",evaluation_metrics:"mAP | Average Heading Similarity (AHS)",conference_journal:"ITSC 2021",source_code:"-"},{key:"23",name:"Robust Small Object Detection on the Water Surface through Fusion of Camera and Millimeter Wave Radar, 23",short_name:"RISFNet",year:2021,task:"Object Detection",annotation:"2D box-level",radar_data_representation:"Point Cloud",projection:"Radar point to image plane",fusion_level:"Feature Level",fusion_operation:"Concatenation | Addition | Multiplication",network:"RISFNet based on CSPdarknet53 and VGG",dataset:"FloW",evaluation_metrics:"-",conference_journal:"ICCV 2020",source_code:"-"},{key:"24",name:"GRIF Net: Gated Region of Interest Fusion Network for Robust 3D Object Detection from Radar Point Cloud and Monocular Image, 24",short_name:"GRIF Net",year:2021,task:"Object Detection",annotation:"3D box-level",radar_data_representation:"Point Cloud",projection:"-",fusion_level:"Feature Level",fusion_operation:"Attention",network:"GRIF Net based on FPN and SBNet",dataset:"nuScenes",evaluation_metrics:"AP",conference_journal:"IROS 2020",source_code:"-"},{key:"25",name:"Fusion Point Pruning for Optimized 2D Object Detection with Radar-Camera Fusion, 25",short_name:"-",year:2021,task:"Object Detection",annotation:"2D box-level",radar_data_representation:"Point Cloud",projection:"Radar point to image plane",fusion_level:"Feature Level",fusion_operation:"Addition | Concatenation",network:"A network based on RetinaNet architecture with a ResNet backbone",dataset:"nuScenes",evaluation_metrics:"-",conference_journal:"WACV 2022",source_code:"-"},{key:"26",name:"A Simple Baseline for BEV Perception Without LiDAR, 26",short_name:"-",year:2021,task:"Semantic Segmentation",annotation:"2D pixel-level",radar_data_representation:"Point Cloud",projection:"Radar point to BEV | Camera image to image plane",fusion_level:"Feature Level",fusion_operation:"Concatenation",network:"A network with a ResNet backbone",dataset:"nuScenes",evaluation_metrics:"IOU",conference_journal:"arXiv",source_code:"-"},{key:"27",name:"RadSegNet: A Reliable Approach to Radar Camera Fusion, 27",short_name:"RadSegNet",year:2022,task:"Object Detection",annotation:"2D box-level | 2D pixel-level",radar_data_representation:"Point Cloud | Range-Azimuth Tensor",projection:"Radar point to BEV | Radar point in 3D Cartesian coordinates",fusion_level:"Data-Level",fusion_operation:"Concatenation",network:"RadSegNet",dataset:"Astyx | RADIATE",evaluation_metrics:"-",conference_journal:"arXiv",source_code:"-"},{key:"28",name:"Bridging the View Disparity of Radar and Camera Features for Multi-modal Fusion 3D Object Detection, 28",short_name:"RCBEV",year:2022,task:"Object Detection",annotation:"3D box-level",radar_data_representation:"Point Cloud",projection:"Radar point to BEV | Image to BEV",fusion_level:"Feature Level",fusion_operation:"Concatenation",network:"RCBEV with Swin Transformer as backbone and FPN as neck",dataset:"nuScenes",evaluation_metrics:"mAP | MTP | NDS",conference_journal:"arXiv",source_code:"-"},{key:"29",name:"CRAFT: Camera-Radar 3D Object Detection with Spatio-Contextual Fusion Transformer, 29",short_name:"CRAFT",year:2022,task:"Object Detection",annotation:"3D box-level",radar_data_representation:"Point Cloud",projection:"-",fusion_level:"Data Level",fusion_operation:"Concatenation",network:"RCBEV with Swin Transformer as backbone and FPN as neck",dataset:"nuScenes",evaluation_metrics:"-",conference_journal:"arXiv",source_code:"-"},{key:"30",name:"DeepFusion: A Robust and Modular 3D Object Detector for Lidars, Cameras and Radar, 30",short_name:"DeepFusion",year:2022,task:"Object Detection",annotation:"3D box-level",radar_data_representation:"Point Cloud",projection:"-",fusion_level:"Feature Level",fusion_operation:"Concatenation",network:"DeepFusion",dataset:"Self-reorded | nuScenes",evaluation_metrics:"-",conference_journal:"arXiv",source_code:"-"},{key:"31",name:"CramNet: Camera-Radar Fusion with Ray-Constrained Cross-Attention for Robust 3D Object Detection, 31",short_name:"CramNet",year:2022,task:"Object Detection",annotation:"3D box-level",radar_data_representation:"Range-Azimuth Tensor",projection:"-",fusion_level:"Feature Level",fusion_operation:"Attention",network:"CramNet",dataset:"RADIATE",evaluation_metrics:"-",conference_journal:"arXiv",source_code:"-"},{key:"32",name:"MVFusion: Multi-View 3D Object Detection with Semantic-aligned Radar and Camera Fusion, 32",short_name:"MVFusion",year:2023,task:"Object Detection",annotation:"3D box-level",radar_data_representation:"Point Cloud",projection:"-",fusion_level:"Feature Level",fusion_operation:"Concatenation | Addition",network:"MVFusion",dataset:"nuScenes",evaluation_metrics:"-",conference_journal:"arXiv",source_code:"-"},{key:"33",name:"CRN: Camera Radar Net for Accurate, Robust, Efficient 3D Perception, 33",short_name:"CRN",year:2023,task:"Object Detection",annotation:"3D box-level",radar_data_representation:"Point Cloud",projection:"-",fusion_level:"Feature Level",fusion_operation:"Concatenation",network:"CRN",dataset:"nuScenes",evaluation_metrics:"-",conference_journal:"arXiv",source_code:"-"}],f=function(o,d,p,u){console.log("params",o,d,p,u)};return(0,e.jsxs)("div",a()(a()(a()({},t),s.wrapper),{},{id:"methods",children:[(0,e.jsx)("div",{className:"title-wrapper",children:(0,e.jsx)("h2",{name:"title",className:"title-h1",children:"Radar-Camera Fusion Methods"})}),(0,e.jsx)(O.Z,{bordered:!0,scroll:{x:"200px"},columns:m,dataSource:v,onChange:f})]}))}}]),r}(C.PureComponent),Qe=null,$e=function(D){R()(r,D);var b=A()(r);function r(){return j()(this,r),b.apply(this,arguments)}return S()(r,[{key:"render",value:function(){var t=Object.assign({},(k()(this.props),this.props)),s=t.dataSource;delete t.dataSource,delete t.isMobile;var m=["S. Chadwick, W. Maddetn, and P. Newman, \u201CDistant vehicle detection using radar and vision,\u201D Proceedings - IEEE International Conference on Robotics and Automation, vol. 2019-May, pp. 8311\u20148317, 2019.","M. Meyer and G. Kuschk, \u201CAstyx: Automotive radar dataset for deep learning based 3D object detection,\u201D EuRAD 2019 - 2019 16th European Radar Conference, pp. 129\u2014132, 2019.","M. Bijelic, T. Gruber, F. Mannan, F. Kraus, W. Ritter, K. Dietmayer, and F. Heide, \u201CSeeing Through Fog Without Seeing Fog: Deep Multimodal Sensor Fusion in Unseen Adverse Weather,\u201D 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), vol. 00, pp. 11 679\u201311 689, 2020.","A. Ouaknine, A. Newson, J. Rebut, F. Tupin, and P. Perez, \u201CCARRADA dataset: Camera and automotive radar with range-Angle-doppler annotations,\u201D arXiv, 2020.","J. Guan, S. Madani, S. Jog, S. Gupta, and H. Hassanieh, \u201CThrough Fog High-Resolution Imaging Using Millimeter Wave Radar,\u201D ser. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2020, pp. 11 461\u201411 470.","M. Mostajabi, C. M. Wang, D. Ranjan, and G. Hsyu, \u201CHigh resolution radar dataset for semi-supervised learning of dynamic objects,\u201D IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, vol. 2020-June, pp. 450\u2014457, 2020.","M. Sheeny, E. D. Pellegrin, S. Mukherjee, A. Ahrabian, S. Wang, and A. Wallace, \u201CRADIATE: A Radar Dataset for Automotive Perception in Bad Weather,\u201D arXiv, 2020.","X. Weng, Y. Man, D. Cheng, J. Park, M. O\u2019Toole, and K. Kitani, \u201CAll-In-One Drive: A Large-Scale Comprehensive Perception Dataset with High-Density Long-Range Point Clouds.\u201D","Y. Wang, G. Wang, H.-M. Hsu, H. Liu, and J.-N. Hwang, \u201CRethinking of Radar\u2019s Role: A Camera-Radar Dataset and Systematic Annotator via Coordinate Alignment,\u201D in CVPRW, 2021.","T.-Y. Lim, S. A. Markowitz, and M. N. Do, \u201CRaDICaL: A Synchronized FMCW Radar, Depth, IMU and RGB Camera Data Dataset with Low-Level FMCW Radar Signals.\u201D","O. Schumann, M. Hahn, N. Scheiner, F. Weishaupt, J. F. Tilly, J. Dickmann, and C. Wohler, \u201CRadarScenes: A Real-World Radar Point Cloud Data Set for Automotive Applications,\u201D 2021. [Online]. Available: http://arxiv.org/abs/2104.02493","A. Zhang, F. E. Nowruzi, and R. Laganiere, \u201CRADDet: Range-Azimuth-Doppler based Radar Object Detection for Dynamic Road Users,\u201D 2021 18th Conference on Robots and Vision (CRV), vol. 00, pp. 95\u2013102, 2021.","Y. Cheng, J. Zhu, M. Jiang, J. Fu, C. Pang1, P. Wang1, K. Sankaran3, O. Onabola3, Y. Liu2, D. Liu3, and Y. Bengio3, \u201CFloW: A Dataset and Benchmark for Floating Waste Detection in Inland Waters,\u201D ser. ICCV, 2021.","J. Rebut, A. Ouaknine, W. Malik, and P. Pe \u0301rez, \u201CRaw High-Definition Radar for Multi-Task Learning,\u201D arXiv, 2021.","A. Palffy, E. Pool, S. Baratam, J. Kooij, and D. Gavrila, \u201CMulti-class Road User Detection with 3+1D Radar in the View-of-Delft Dataset,\u201D IEEE Robotics and Automation Letters, vol. PP, no. 99, pp. 1\u20131, 2022.","K. Burnett, D. J. Yoon, Y. Wu, A. Z. Li, H. Zhang, S. Lu, J. Qian, W.-K. Tseng, A. Lambert, K. Y. K. Leung, A. P. Schoellig, and T. D. Barfoot, \u201CBoreas: A Multi-Season Autonomous Driving Dataset,\u201D arXiv, 2022.","A. Palffy, E. Pool, S. Baratam, J. Kooij, and D. Gavrila, \u201CMulti-class Road User Detection with 3+1D Radar in the View-of-Delft Dataset,\u201D IEEE Robotics and Automation Letters, vol. PP, no. 99, pp. 1\u20131, 2022.","D.-H. Paek, S.-H. Kong, and K. T. Wijaya, \u201CK-Radar: 4D Radar Object Detection Dataset and Benchmark for Autonomous Driving in Various Weather Conditions,\u201D arXiv, 2022.","T.Matuszka,I.Barton,A \u0301.Butykai,P.Hajas,D.Kiss,D.Kova \u0301cs, S. Kunsa \u0301gi-Ma \u0301te \u0301, P. Lengyel, G. Ne \u0301meth, L. Peto \u030B et al., \u201Caimotive dataset: A multimodal dataset for robust autonomous driving with long-range perception,\u201D arXiv preprint arXiv:2211.09445, 2022."],v=["S. Chadwick, W. Maddetn, and P. Newman, \u201CDistant vehicle detection using radar and vision,\u201D Proceedings - IEEE International Conference on Robotics and Automation, vol. 2019-May, pp. 8311\u20148317, 2019.","R. Nabati and H. Qi, \u201CRRPN: Radar Region Proposal Network for Object Detection in Autonomous Vehicles,\u201D arXiv, pp. 3093\u20143097, 2019.","H. Jha, V. Lodhi, and D. Chakravarty, \u201CObject Detection and Identification Using Vision and Radar Data Fusion System for Ground-Based Navigation,\u201D 2019 6th International Conference on Signal Processing and Integrated Networks (SPIN), vol. 00, pp. 590\u2013593, 2019.","V. Lekic and Z. Babic, \u201CAutomotive radar and camera fusion using Generative Adversarial Networks,\u201D Computer Vision and Image Understanding, vol. 184, no. July 2018, pp. 1\u20148, 2019. [Online]. Available: https://doi.org/10.1016/j.cviu.2019.04.002","M. Meyer and G. Kuschk, \u201CDeep Learning Based 3D Object Detection for Automotive Radar and Camera,\u201D 1 2019.","V. John and S. Mita, \u201CRVNet: Deep Sensor Fusion of Monocular Camera and Radar for Image-Based Obstacle Detection in Challenging Environments,\u201D Lecture Notes in Computer Science, pp. 351\u2013364, 2019.","T.-Y. Lim, A. Ansari, B. Major, and D. Fontijne, \u201CRadar and Camera Early Fusion for Vehicle Detection in Advanced Driver Assistance Systems.\u201D","V. John, M. K. Nithilan, S. Mita, H. Tehrani, R. S. Sudheesh, and P. P. Lalu, \u201CSO-Net: Joint Semantic Segmentation and Obstacle Detection Using Deep Fusion of Monocular Camera and Radar,\u201D Lecture Notes in Computer Science, pp. 138\u2013148, 2020.","S. Chang, Y. Zhang, F. Zhang, X. Zhao, S. Huang, Z. Feng, and Z. Wei, \u201CSpatial Attention Fusion for Obstacle Detection Using MmWave Radar and Vision Sensor,\u201D Sensors, vol. 20, no. 4, p. 956, 2020.","F. Nobis, M. Geisslinger, M. Weber, J. Betz, and M. Lienkamp, \u201CA Deep learning-based radar and camera sensor fusion architecture for object detection,\u201D arXiv, 2020.","M. Bijelic, T. Gruber, F. Mannan, F. Kraus, W. Ritter, K. Dietmayer, and F. Heide, \u201CSeeing Through Fog Without Seeing Fog: Deep Multimodal Sensor Fusion in Unseen Adverse Weather,\u201D 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), vol. 00, pp. 11 679\u201311 689, 2020.","R. Yadav, A. Vierling, and K. Berns, \u201CRadar+RGB Attentive Fusion for Robust Object Detection in Autonomous Vehicles,\u201D arXiv, 2020.","R. Nabati and H. Qi, \u201CRadar-camera sensor fusion for joint object detection and distance estimation in autonomous vehicles,\u201D arXiv, 2020.","K. Kowol, M. Rottmann, S. Bracke, and H. Gottschalk, \u201CYOdar: Uncertainty-based Sensor Fusion for Vehicle Detection with Camera and Radar Sensors,\u201D arXiv, 2020.","R. Nabati and H. Qi, \u201CRadar-camera sensor fusion for joint object detection and distance estimation in autonomous vehicles,\u201D arXiv, 2020.","Y. Wang, Z. Jiang, X. Gao, J.-N. Hwang, G. Xing, and H. Liu, \u201CRODNet: Radar Object Detection using Cross-Modal Supervision,\u201D 2021 IEEE Winter Conference on Applications of Computer Vision(WACV), vol. 00, pp. 504\u2013513, 2021.","X. Gao, G. Xing, S. Roy, and H. Liu, \u201CRAMP-CNN: A Novel Neural Network for Enhanced Automotive Radar Object Recognition,\u201D IEEE Sensors Journal, vol. 21, no. 4, pp. 5119\u20145132, 2021.","L.-q. Li and Y.-l. Xie, \u201CA Feature Pyramid Fusion Detection Algorithm Based on Radar and Camera Sensor,\u201D 2020 15th IEEE International Conference on Signal Processing (ICSP), vol. 1, pp. 366\u2013370, 2020.","J. Kim, Y. Kim, and D. Kum, \u201CLow-level Sensor Fusion for 3D Vehicle Detection using Radar Range-Azimuth Heatmap and Monocular Image,\u201D Lecture Notes in Computer Science, pp. 388\u2013402, 2021.","X. Dong, B. Zhuang, Y. Mao, and L. Liu, \u201CRadar Camera Fusion via Representation Learning in Autonomous Driving,\u201D 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), vol. 00, pp. 1672\u20131681, 2021.","F. Nobis, E. Shafiei, P. Karle, J. Betz, and M. Lienkamp, \u201CRadar Voxel Fusion for 3D Object Detection,\u201D Applied Sciences, vol. 11, no. 12, p. 5598, 2021.","H. Cui, J. Wu, J. Zhang, G. Chowdhary, and W. R. Norris, \u201C3D Detection and Tracking for On-road Vehicles with a Monovision Camera and Dual Low-cost 4D mmWave Radars,\u201D 2021 IEEE International Intelligent Transportation Systems Conference (ITSC), vol. 00, pp. 2931\u20132937, 2021.","Y. Cheng, H. Xu, and Y. Liu, \u201CRobust Small Object Detection on the Water Surface through Fusion of Camera and Millimeter Wave Radar,\u201D ser. ICCV, 2021.","Y. Kim, J. W. Choi, and D. Kum, \u201CGRIF Net: Gated Region of Interest Fusion Network for Robust 3D Object Detection from Radar Point Cloud and Monocular Image,\u201D 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), vol. 00, pp. 10 857\u201310 864, 2021.","L. Sta \u0308cker, P. Heidenreich, J. Rambach, and D. Stricker, \u201CFusion Point Pruning for Optimized 2D Object Detection with Radar-Camera Fusion,\u201D 2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), vol. 00, pp. 1275\u20131282, 2022.","A. W. Harley, Z. Fang, J. Li, R. Ambrus, and K. Fragkiadaki, \u201CA Simple Baseline for BEV Perception Without LiDAR,\u201D arXiv, 2022.","K. Bansal, K. Rungta, and D. Bharadia, \u201CRadSegNet: A Reliable Approach to Radar Camera Fusion,\u201D arXiv, 2022.","T. Zhou, Y. Shi, J. Chen, K. Jiang, M. Yang, and D. Yang, \u201CBridging the View Disparity of Radar and Camera Features for Multi-modal Fusion 3D Object Detection,\u201D arXiv, 2022.","Y. Kim, S. Kim, J. W. Choi, and D. Kum, \u201CCRAFT: Camera-Radar 3D Object Detection with Spatio-Contextual Fusion Transformer,\u201D arXiv, 2022.","F. Drews, D. Feng, F. Faion, L. Rosenbaum, M. Ulrich, and C. Gla \u0308ser, \u201CDeepFusion: A Robust and Modular 3D Object Detector for Lidars, Cameras and Radars,\u201D arXiv, 2022.","J.-J. Hwang, H. Kretzschmar, J. Manela, S. Rafferty, N. ArmstrongCrews, T. Chen, and D. Anguelov, \u201CCramNet: Camera-Radar Fusion with Ray-Constrained Cross-Attention for Robust 3D Object Detection,\u201D arXiv, 2022.","Z. Wu, G. Chen, Y. Gan, L. Wang, and J. Pu, \x93Mvfusion: Multi-view 3d object detection with semantic-aligned radar and camera fusion,\x94 arXiv preprint arXiv:2302.10511, 2023.","Y. Kim, S. Kim, J. Shin, J. W. Choi, and D. Kum, \x93Crn: Camera radar net for accurate, robust, efAcient 3d perception,\x94 arXiv preprint arXiv:2304.00670, 2023."],f=m.map(function(o,d){return(0,e.jsxs)("p",{children:["[",d+1,"] ",o]})}),c=v.map(function(o,d){return(0,e.jsxs)("p",{children:["[",d+1+m.length,"] ",o]})});return(0,e.jsx)("div",{className:"home-page-wrapper content12-wrapper",id:"references",children:(0,e.jsxs)("div",{className:"content12",children:[(0,e.jsx)("h1",{name:"title",class:"title-h1",children:"Citation"}),(0,e.jsx)("div",{style:{backgroundColor:"#f3f6fa",padding:"10px"},children:(0,e.jsxs)("code",{children:["@misc{yao2023radarcamera,",(0,e.jsx)("br",{}),"\xA0\xA0\xA0\xA0","title={Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review}, ",(0,e.jsx)("br",{}),"\xA0\xA0\xA0\xA0","author={Shanliang Yao and Runwei Guan and Xiaoyu Huang and Zhuoxiao Li and Xiangyu Sha and Yong Yue and Eng Gee Lim and Hyungjoon Seo and Ka Lok Man and Xiaohui Zhu and Yutao Yue},",(0,e.jsx)("br",{}),"\xA0\xA0\xA0\xA0","year={2023},",(0,e.jsx)("br",{}),"\xA0\xA0\xA0\xA0","eprint={2304.10410},",(0,e.jsx)("br",{}),"\xA0\xA0\xA0\xA0","archivePrefix={arXiv},",(0,e.jsx)("br",{}),"\xA0\xA0\xA0\xA0","primaryClass={cs.CV}",(0,e.jsx)("br",{}),"}"]})})]})})}}]),r}(C.PureComponent),qe=null,Ne=function(D){R()(r,D);var b=A()(r);function r(){return j()(this,r),b.apply(this,arguments)}return S()(r,[{key:"render",value:function(){var t=Object.assign({},(k()(this.props),this.props)),s=t.dataSource;return delete t.dataSource,delete t.isMobile,(0,e.jsx)(I(),a()(a()(a()({},t),s.OverPack),{},{children:(0,e.jsx)(T.Z,a()(a()({type:"bottom",leaveReverse:!0,delay:[0,100]},s.titleWrapper),{},{children:s.titleWrapper.children.map(L)}),"page")}))}}]),r}(C.PureComponent),_e=Ne,Pe=function(D){R()(r,D);var b=A()(r);function r(){return j()(this,r),b.apply(this,arguments)}return S()(r,[{key:"render",value:function(){var t=t||[];(function(){var v=document.createElement("script");v.src="https://hm.baidu.com/hm.js?ccbb089c76eb88b55f087dfb0b7ecc03";var f=document.getElementsByTagName("script")[0];f.parentNode.insertBefore(v,f)})();var s=Object.assign({},(k()(this.props),this.props)),m=s.dataSource;return delete s.dataSource,delete s.isMobile,(0,e.jsx)("div",a()(a()(a()({},s),m.wrapper),{},{children:(0,e.jsx)(I(),a()(a()({},m.OverPack),{},{children:(0,e.jsx)(F.ZP,a()(a()({animation:{y:"+=30",opacity:0,type:"from"}},m.copyright),{},{children:m.copyright.children}),"footer")}))}))}}]),r}(C.PureComponent),we=Pe,Oe=l.p+"static/logo.a8ce2897.png",Fe={isScrollLink:!0,wrapper:{className:"header2 home-page-wrapper jrhtw9ph4a-editor_css"},page:{className:"home-page"},logo:{className:"header2-logo",children:Oe},LinkMenu:{className:"header2-menu",children:[{name:"linkNav",to:"WaterScenes",children:"WaterScenes Dataset",className:"menu-item"},{name:"linkNav",to:"USV",children:"USV Setup",className:"menu-item"},{name:"linkNav",to:"Citation",children:"Citation",className:"menu-item"}]},mobileMenu:{className:"header2-mobile-menu"},Menu:{children:[{name:"Banner3_0",to:"Banner3_0",children:"\u9996\u9875",className:"active menu-item"},{name:"Content8_0",to:"Content8_0",children:"\u7279\u9080\u5609\u5BBE",className:"menu-item"},{name:"Content9_0",to:"Content9_0",children:"\u4F1A\u8BAE\u65E5\u7A0B",className:"menu-item"},{name:"Content10_0",to:"Content10_0",children:"\u5927\u4F1A\u5730\u5740",className:"menu-item"},{name:"Content11_0",to:"Content11_0",children:"\u5C55\u53F0\u5C55\u793A",className:"menu-item"},{name:"Content12_0",to:"Content12_0",children:"\u7279\u522B\u9E23\u8C22",className:"menu-item"}]}},Me={wrapper:{className:"banner3"},textWrapper:{className:"banner3-text-wrapper",children:[{name:"slogan",className:"banner3-slogan",children:"WaterScenes: A Multi-Task 4D Radar-Camera Fusion Dataset for Autonomous Driving on Water Surfaces"},{name:"nameEn",className:"banner3-name-en",children:"Shanliang Yao, Runwei Guan, Zhaodong Wu, Yi Ni, Zixian Zhang, Zile Huang"},{name:"nameEn",className:"banner3-name-en",children:"Xiaohui Zhu, Yutao Yue, Yong Yue, Hyungjoon Seo, Ka Lok Man"},{name:"time",className:"banner3-time",children:"University of Liverpool, Xi\u2018an Jiaotong-Liverpool University, Institute of Deep Perception Technology, JITRI"},{name:"button",className:"banner3-button",children:"GitHub: https://github.com/WaterScenes",type:"primary",href:"https://github.com/WaterScenes",target:"_blank"}]}},Te={OverPack:{className:"home-page-wrapper content13-wrapper",playScale:.3},titleWrapper:{className:"title-wrapper",children:[]}},Ie={wrapper:{className:"home-page-wrapper footer0-wrapper"},OverPack:{className:"home-page footer0",playScale:.01},copyright:{className:"copyright",children:(0,e.jsxs)("span",{children:["\xA92023 ",(0,e.jsx)("a",{href:"https://github.com/WaterScenes",children:"WaterScenes"})," All Rights Reserved"]})}},ea={wrapper:{className:"home-page-wrapper content6-wrapper"},OverPack:{className:"home-page content6"},textWrapper:{className:"content6-text",xs:24,md:8},titleWrapper:{className:"title-wrapper",children:[{name:"title",children:"\u8682\u8681\u91D1\u878D\u4E91\u63D0\u4F9B\u4E13\u4E1A\u7684\u670D\u52A1",className:"title-h1"},{name:"content",className:"title-content",children:"\u57FA\u4E8E\u963F\u91CC\u4E91\u8BA1\u7B97\u5F3A\u5927\u7684\u57FA\u7840\u8D44\u6E90"}]},img:{children:"https://zos.alipayobjects.com/rmsportal/VHGOVdYyBwuyqCx.png",className:"content6-img",xs:24,md:16},block:{children:[{name:"block0",img:{children:"https://zos.alipayobjects.com/rmsportal/NKBELAOuuKbofDD.png",className:"content6-icon"},title:{className:"content6-title",children:"\u6280\u672F"},content:{className:"content6-content",children:"\u4E30\u5BCC\u7684\u6280\u672F\u7EC4\u4EF6\uFF0C\u7B80\u5355\u7EC4\u88C5\u5373\u53EF\u5FEB\u901F\u642D\u5EFA\u91D1\u878D\u7EA7\u5E94\u7528\uFF0C\u4E30\u5BCC\u7684\u6280\u672F\u7EC4\u4EF6\uFF0C\u7B80\u5355\u7EC4\u88C5\u5373\u53EF\u5FEB\u901F\u642D\u5EFA\u91D1\u878D\u7EA7\u5E94\u7528\u3002"}},{name:"block1",img:{className:"content6-icon",children:"https://zos.alipayobjects.com/rmsportal/xMSBjgxBhKfyMWX.png"},title:{className:"content6-title",children:"\u878D\u5408"},content:{className:"content6-content",children:"\u89E3\u653E\u4E1A\u52A1\u53CA\u6280\u672F\u751F\u4EA7\u529B\uFF0C\u63A8\u52A8\u91D1\u878D\u670D\u52A1\u5E95\u5C42\u521B\u65B0\uFF0C\u63A8\u52A8\u91D1\u878D\u670D\u52A1\u5E95\u5C42\u521B\u65B0\u3002\u89E3\u653E\u4E1A\u52A1\u53CA\u6280\u672F\u751F\u4EA7\u529B\uFF0C\u63A8\u52A8\u91D1\u878D\u670D\u52A1\u5E95\u5C42\u521B\u65B0\u3002"}},{name:"block2",img:{className:"content6-icon",children:"https://zos.alipayobjects.com/rmsportal/MNdlBNhmDBLuzqp.png"},title:{className:"content6-title",children:"\u5F00\u53D1"},content:{className:"content6-content",children:"\u7B26\u5408\u91D1\u878D\u53CA\u8981\u6C42\u7684\u5B89\u5168\u53EF\u9760\u3001\u9AD8\u53EF\u7528\u3001\u9AD8\u6027\u80FD\u7684\u670D\u52A1\u80FD\u529B\uFF0C\u7B26\u5408\u91D1\u878D\u53CA\u8981\u6C42\u7684\u5B89\u5168\u53EF\u9760\u3001\u9AD8\u53EF\u7528\u3001\u9AD8\u6027\u80FD\u7684\u670D\u52A1\u80FD\u529B\u3002"}}]}},Le={wrapper:{className:"home-page-wrapper"},OverPack:{className:"home-page",playScale:.05},copyright:{className:"copyright",children:(0,e.jsxs)("span",{children:["\xA92018 ",(0,e.jsx)("a",{href:"https://motion.ant.design",children:"Ant Motion"})," All Rights Reserved"]})}},aa={wrapper:{className:"home-page-wrapper"},OverPack:{className:"home-page",playScale:.05},copyright:{className:"copyright",children:(0,e.jsxs)("span",{children:["\xA92018 ",(0,e.jsx)("a",{href:"https://motion.ant.design",children:"Ant Motion"})," All Rights Reserved"]})}},na={wrapper:{className:"home-page-wrapper content12-wrapper"},OverPack:{className:"home-page content12",playScale:.05}},ta={wrapper:{className:"home-page-wrapper pricing2-wrapper"},page:{className:"home-page pricing2"},OverPack:{playScale:.3,className:"pricing2-content-wrapper"},titleWrapper:{className:"pricing2-title-wrapper",children:[{name:"title",children:"Comparison of Different Sensors",className:"pricing2-title-h1"}]},Table:{name:"tabsTitle",size:"default",className:"pricing2-table",columns:{children:[{dataIndex:"name",key:"name",name:"empty",childWrapper:{children:[{name:"name",children:" "},{name:"content",children:" "}]}},{dataIndex:"free",key:"free",name:"free",childWrapper:{className:"pricing2-table-name-block",children:[{name:"name",className:"pricing2-table-name",children:(0,e.jsx)("span",{children:(0,e.jsxs)("p",{children:[(0,e.jsx)("span",{children:"Camera"}),(0,e.jsx)("br",{})]})})}]}},{dataIndex:"basic",key:"basic",name:"basic",childWrapper:{className:"pricing2-table-name-block",children:[{name:"name",className:"pricing2-table-name",children:(0,e.jsx)("span",{children:(0,e.jsx)("span",{children:(0,e.jsx)("p",{children:"Radar"})})})}]}},{dataIndex:"pro",key:"pro",name:"pro",childWrapper:{className:"pricing2-table-name-block",children:[{name:"name",className:"pricing2-table-name",children:(0,e.jsx)("span",{children:(0,e.jsx)("p",{children:"LiDAR"})})}]}}]},dataSource:{children:[{name:"list0",children:[{className:"pricing2-table-content-name",name:"name",children:"Color, Texture, Shape"},{name:"content1",children:"images/start-fill.svg",className:"pricing2-table-content"},{children:"Unlimited",name:"content1",className:"pricing2-table-content"},{children:"Unlimited",name:"content2",className:"pricing2-table-content"},{children:"Unlimited",name:"content3",className:"pricing2-table-content"}]},{name:"list1",children:[{className:"pricing2-table-content-name",name:"name",children:"Range Measurement"},{children:"Limited",name:"content0",className:"pricing2-table-content"},{children:"Unlimited",name:"content1",className:"pricing2-table-content"},{children:"Unlimited",name:"content2",className:"pricing2-table-content"},{children:"Unlimited",name:"content3",className:"pricing2-table-content"}]},{name:"list2",children:[{className:"pricing2-table-content-name",name:"name",children:"Velocity Measurement"},{name:"content0",children:"50GB",className:"pricing2-table-content"},{name:"content1",children:"250GB",className:"pricing2-table-content"},{name:"content2",children:"600GB",className:"pricing2-table-content"},{name:"content3",children:"Unlimited",className:"pricing2-table-content"}]},{name:"list3",children:[{className:"pricing2-table-content-name",name:"name",children:"Lighting Robustness"},{children:"-",name:"content0",className:"pricing2-table-content"},{name:"content1",children:"https://gw.alipayobjects.com/zos/basement_prod/14ce3060-34e6-4b30-9a45-1a6b95542310.svg",className:"pricing2-table-content"},{name:"content2",children:"https://gw.alipayobjects.com/zos/basement_prod/14ce3060-34e6-4b30-9a45-1a6b95542310.svg",className:"pricing2-table-content"},{name:"content3",children:"https://gw.alipayobjects.com/zos/basement_prod/14ce3060-34e6-4b30-9a45-1a6b95542310.svg",className:"pricing2-table-content"}]},{name:"list4",children:[{className:"pricing2-table-content-name",name:"name",children:"Weather Robustness"},{name:"content0",children:"-",className:"pricing2-table-content"},{name:"content1",children:"-",className:"pricing2-table-content"},{name:"content2",children:"https://gw.alipayobjects.com/zos/basement_prod/14ce3060-34e6-4b30-9a45-1a6b95542310.svg",className:"pricing2-table-content"},{name:"content3",children:"https://gw.alipayobjects.com/zos/basement_prod/14ce3060-34e6-4b30-9a45-1a6b95542310.svg",className:"pricing2-table-content"}]},{name:"list5",children:[{className:"pricing2-table-content-name",name:"name",children:"Classification Ability"},{name:"content0",children:"-",className:"pricing2-table-content"},{name:"content1",children:"-",className:"pricing2-table-content"},{name:"content2",children:"-",className:"pricing2-table-content"},{name:"content3",children:"https://gw.alipayobjects.com/zos/basement_prod/14ce3060-34e6-4b30-9a45-1a6b95542310.svg",className:"pricing2-table-content"}]},{name:"list5",children:[{className:"pricing2-table-content-name",name:"name",children:"3D Perception"},{name:"content0",children:"-",className:"pricing2-table-content"},{name:"content1",children:"-",className:"pricing2-table-content"},{name:"content2",children:"-",className:"pricing2-table-content"},{name:"content3",children:"https://gw.alipayobjects.com/zos/basement_prod/14ce3060-34e6-4b30-9a45-1a6b95542310.svg",className:"pricing2-table-content"}]},{name:"list5",children:[{className:"pricing2-table-content-name",name:"name",children:"System Cost"},{name:"content0",children:"-",className:"pricing2-table-content"},{name:"content1",children:"-",className:"pricing2-table-content"},{name:"content2",children:"-",className:"pricing2-table-content"},{name:"content3",children:"https://gw.alipayobjects.com/zos/basement_prod/14ce3060-34e6-4b30-9a45-1a6b95542310.svg",className:"pricing2-table-content"}]}]}}},Y;(0,G.ac)(function(D){Y=D});var Ve=typeof window!="undefined"?window:{},J=Ve.location,Q=J===void 0?{}:J,We=function(D){R()(r,D);var b=A()(r);function r(y){var t;return j()(this,r),t=b.call(this,y),t.state={isMobile:Y,show:!Q.port},t}return S()(r,[{key:"componentDidMount",value:function(){var t=this;(0,G.ac)(function(s){t.setState({isMobile:!!s})}),Q.port&&setTimeout(function(){t.setState({show:!0})},500)}},{key:"render",value:function(){var t=this,s=[(0,e.jsx)(le,{id:"Nav0_0",dataSource:Fe,isMobile:this.state.isMobile},"Nav0_0"),(0,e.jsx)(he,{id:"Banner3_0",dataSource:Me,isMobile:this.state.isMobile},"Banner3_0"),(0,e.jsx)(Ae,{id:"Dataset0_0",dataSource:Le,isMobile:this.state.isMobile},"Dataset0_0"),(0,e.jsx)(_e,{id:"Content13_0",dataSource:Te,isMobile:this.state.isMobile},"Content13_0"),(0,e.jsx)(we,{id:"Footer0_0",dataSource:Ie,isMobile:this.state.isMobile},"Footer0_0")];return(0,e.jsx)("div",{className:"templates-wrapper",ref:function(v){t.dom=v},children:this.state.show&&s})}}]),r}(C.Component)}}]);
